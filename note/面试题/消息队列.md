## MQ高频面试题汇总
————持续更新!  
<a href="#1">1.如何进行消息队列的技术选型?</a>  
<a href="#2">2.消息队列有什么优点和缺点啊?</a>  
<a href="#3">3.常用的MQ的对比？</a>  
<a href="#4">4.如何保证消息队列的高可用啊？</a>  
<a href="#5">5.如何保证消息不被重复消费啊（如何保证消息消费时的幂等性）？</a>  
<a href="#6">6.如何保证消息的可靠性传输（如何处理消息丢失的问题）？</a>  
<a href="#7">7.如何保证消息的顺序性？</a>  
<a href="#8">8.如何解决消息队列的延时以及过期失效问题？</a>  
<a href="#9">9.消息队列满了以后该怎么处理？</a>  
<a href="#10">10.有几百万消息持续积压几小时，说说怎么解决？</a>  

## <a name="1">1.如何进行消息队列的技术选型</a>
### 解耦

**不用MQ的系统耦合场景**

![](https://github.com/lvCmx/study/blob/master/note/面试题/resource/不用mq的系统.png)

说明：A系统发送数据到BCDEF各个系统中，接口调用发送，那么此时，如果出现一个系统G也需要这个数据呢？那如果D系统现在不需要了呢？现在A系统又要发送第二种数据了呢？A系统负责人估计已经崩溃了。另外A系统时时刻刻考虑BCDEF系统如果挂了怎么办？要不要重新发送数据等问题。

**使用mq的系统**

![](https://github.com/lvCmx/study/blob/master/note/面试题/resource/mq系统.png)

你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用MQ给他异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个MQ去进行系统的解耦。在简历中体现出来这块东西，用MQ作解耦。

### 异步

**不使用mq的系统**
![](https://github.com/lvCmx/study/blob/master/note/面试题/resource/异步-不用mq.png)
A系统接收一个请求，需要在自己本地写库，还需要在BCD三个系统写库，自己本地写库要3ms，BCD三个系统分别写库要300ms、450ms、200ms。最终请求总延时是3 + 300 + 450 + 200 = 953ms，接近1s，用户感觉搞个什么东西，慢死了慢死了。
![](https://github.com/lvCmx/study/blob/master/note/面试题/resource/mq中的异步系统.png)
### 削峰
**没有mq削值把mysql压崩**
![](https://github.com/lvCmx/study/blob/master/note/面试题/resource/削峰-mq01.png)
例如：我们现在有这样一个需要，系统中午的时候，访问的用户比较多，当每秒要操作2000条以上的sql语句时，mysql可能支撑不住，会把数据压跨。
**用mq削峰**
![](https://github.com/lvCmx/study/blob/master/note/面试题/resource/mq削峰.png)
## <a name="2">2.消息队列有什么优点和缺点啊?</a>
优点就是上面所说的三点：解耦、异步、削峰
缺点：
- 系统可用性降低：系统引入的外部依赖越多，越容易挂掉，本来你就是A系统调用BCD三个系统的接口就好了，人ABCD四个系统好好的，没啥问题，你偏加个MQ进来，万一MQ挂了咋整？MQ挂了，整套系统崩溃了，你不就完了么。
- 系统复杂性提高：硬生生加个MQ进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已
- 一致性问题：A系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是BCD三个系统那里，BD两个系统写库成功了，结果C系统写库失败了，咋整？你这数据就不一致了。
## <a name="3">3.常用的MQ的对比？</a>

## <a name="4">4.如何保证消息队列的高可用啊？</a>
### RabbitMQ保证高可用
**三种模式**

- 单一模式：即单机情况不做集群，就单独运行一个rabbitMQ而已。

- 集群模式：

  ![](https://github.com/lvCmx/study/blob/master/note/面试题/resource/mq集群模式.png)

  以两台机器(A/B)为例来进行说明，对于Queue来说，消息实体只存在于其中一个节点，而A或B两个节点仅有相同的元烽据，即队列的结构。当消息进入A节点的Queue后，consumer从B节点消费时，RabbitMQ会面临在A、B间进行消息传输，把A中的消息实体取出并经过B发送给consumer,所以consumer应尽量连接每一个节点，从中取消息。即对于同一个逻辑队列，要在多个节点建立物理 Queue。否则无论consumer连A/B，出口总在A，会产生瓶颈。当A节点出现故障后，B节点无法取到A节点中还未消费的消息实体。如果做了消息持久化，那么得A节点恢复，然后才可被消费。如果没有持久化的话，就会产生消息丢失的现象。

- 镜像模式：

  ![](https://github.com/lvCmx/study/blob/master/note/面试题/resource/镜像模型.png)

  这种模式才是所谓的rabbitMQ的高可用模式，跟普通集群模式不一样的是，你创建的queue，无论元数据还是queue里的消息都会存在于多个实例上，然后每次你写消息到queue的时候，都会自动把消息同步到多个实例的queue里进行消息同步。这样的话，当一台机器宕机了，别的机器都可以用。坏外在于，性能开销太大。另外如果某个queue负载很重，你加机器 ，新增的机器也包含了这个queue的所有的数据，并没有办法扩展你的queue。

**客户端连接RabbitMQ集群策略**

### Kafka高可用
![](https://github.com/lvCmx/study/blob/master/note/面试题/resource/kafka高可用.png)

kafka0.8以后，提供了HA机制，就是replica(复制品)副本机制，每个partition的数据都会同步到其它机器上，形成自己的多个replica副本。所有replica会选举一个leader出来，那么生产和消费都跟这个leader打交道，然后其他replica就是follower。写的时候leader会负责把数据同步到所有follower上去，读的时候就直接读leader上的数据即可。只能从leader操作，原因是保证数据一致性问题。

**可靠性是如何保证的？**

1、数据的副本
2、如果leader宕机了，那么此时会从follower中重新选举一个新的leader出来。大家继续读写那个新的leader即可。
3、写数据的时候，生产者就写leader，然后leader将数据落地写本地磁盘，接着其他follower自己主动从leader来pull数据。一旦所有follower同步好数据了，就会发送ack给leader，leader收到所有follower的ack之后，就会crcp回写成功的消息给生产者。
## <a name="5">5.如何保证消息不被重复消费啊（如何保证消息消费时的幂等性）？</a>

有时候无法保证MQ只发一次消息，所以，做预防的时候，需要我们自己根据自己的实际业务来处理。

以kafka为例讲解此问题。

**如何保证消息不被重复消费啊？**

kafka实际上有个offset的概念，就是每个消息写进去，都有一个offset，代表他的序号，然后consumer消费了数据之后，每隔一段时间，会把自己消费过的消息的offset提交一下，代表我已经消费过了，下次我要是重启服务的话，你就让我接着从上次消费到的offset来继续消费。其中这个offset保存在zookeeper中。

另外如果在重启服务的时候，直接kill进程了，再重启。这会导致consumer有些消息处理了，但是没有来得及提交offset，重启之后，少数消息会再次消费一次。

给你举个例子吧。假设你有个系统，消费一条往数据库里插入一条，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？但是你要是消费到第二次的时候，自己判断一下已经消费过了，直接扔了，不就保留了一条数据？

一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性

 **如何保证消息消费时的幂等性？**

幂等性，我通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。

其实还是得结合业务来思考，我这里给几个思路：

（1）比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update一下好吧

（2）比如你是写redis，那没问题了，反正每次都是set，天然幂等性

（3）比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的id，类似订单id之类的东西，然后你这里消费到了之后，先根据这个id去比如redis里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个id写redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。

还有比如基于数据库的唯一键来保证重复数据不会重复插入多条，我们之前线上系统就有这个问题，就是拿到数据的时候，每次重启可能会有重复，因为kafka消费者还没来得及提交offset，重复数据拿到了以后我们插入的时候，因为有唯一键约束了，所以重复数据只会插入报错，不会导致数据库中出现脏数据

如何保证MQ的消费是幂等性的，需要结合具体的业务来看

## <a name="6">6.如何保证消息的可靠性传输（如何处理消息丢失的问题）？</a>

这个丢数据，mq一般分为两种，要么是mq自己弄丢了，要么是我们消费的时候弄丢了。咱们从rabbitmq和kafka分别来分析一下吧

### RabbitMQ可靠性传输

**生产者弄丢数据**

生产者将数据发送到rabbitmq的时候，可能数据就在半路给搞丢了，因为网络啥的问题，都有可能。

解决方案1：

此时可以选择用rabbitmq提供的事务功能，就是生产者发送数据之前开启rabbitmq事务（channel.txSelect），然后发送消息，如果消息没有成功被rabbitmq接收到，那么生产者会收到异常报错，此时就可以回滚事务（channel.txRollback），然后重试发送消息；如果收到了消息，那么可以提交事务（channel.txCommit）。但是问题是，rabbitmq事务机制一搞，基本上吞吐量会下来，因为太耗性能。

解决方案2：

​	所以一般来说，如果你要确保说写rabbitmq的消息别丢，可以开启confirm模式，在生产者那里设置开启confirm模式之后，你每次写的消息都会分配一个唯一的id，然后如果写入了rabbitmq中，rabbitmq会给你回传一个ack消息，告诉你说这个消息ok了。如果rabbitmq没能处理这个消息，会回调你一个nack接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息id的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。

​	事务机制和cnofirm机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是confirm机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息rabbitmq接收了之后会异步回调你一个接口通知你这个消息接收到了。

**RabbitMQ丢消息**

就是rabbitmq自己弄丢了数据，这个你必须开启rabbitmq的持久化，就是消息写入之后会持久化到磁盘，哪怕是rabbitmq自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，rabbitmq还没持久化，自己就挂了，可能导致少量数据会丢失的，但是这个概率较小。

设置持久化有两个步骤，第一个是创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里的数据；第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去。必须要同时设置这两个持久化才行，rabbitmq哪怕是挂了，再次重启，也会从磁盘上重启恢复queue，恢复这个queue里的数据。

而且持久化可以跟生产者那边的confirm机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者ack了，所以哪怕是在持久化到磁盘之前，rabbitmq挂了，数据丢了，生产者收不到ack，你也是可以自己重发的。

哪怕是你给rabbitmq开启了持久化机制，也有一种可能，就是这个消息写到了rabbitmq中，但是还没来得及持久化到磁盘上，结果不巧，此时rabbitmq挂了，就会导致内存里的一点点数据会丢失。

**消费端弄丢了数据**

rabbitmq如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，rabbitmq认为你都消费了，这数据就丢了。

这个时候得用rabbitmq提供的ack机制，简单来说，就是你关闭rabbitmq自动ack，可以通过一个api来调用就行，然后每次你自己代码里确保处理完的时候，再程序里ack一把。这样的话，如果你还没处理完，不就没有ack？那rabbitmq就认为你还没处理完，这个时候rabbitmq会把这个消费分配给别的consumer去处理，消息是不会丢的。

### kafka消息的可靠性

**消费端弄丢了数据**

唯一可能导致消费者弄丢数据的情况，就是说，你那个消费到了这个消息，然后消费者那边自动提交了offset，让kafka以为你已经消费好了这个消息，其实你刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。

这不是一样么，大家都知道kafka会自动提交offset，那么只要关闭自动提交offset，在处理完之后自己手动提交offset，就可以保证数据不会丢。但是此时确实还是会重复消费，比如你刚处理完，还没提交offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。

生产环境碰到的一个问题，就是说我们的kafka消费者消费到了数据之后是写到一个内存的queue里先缓冲一下，结果有的时候，你刚把消息写入内存queue，然后消费者会自动提交offset。

然后此时我们重启了系统，就会导致内存queue里还没来得及处理的数据就丢失了

**kafka弄丢了数据**

![](https://github.com/lvCmx/study/blob/master/note/面试题/resource/kafka高可靠01.png)

这块比较常见的一个场景，就是kafka某个broker宕机，然后重新选举partiton的leader时。大家想想，要是此时其他的follower刚好还有些数据没有同步，结果此时leader挂了，然后选举某个follower成leader之后，他不就少了一些数据？这就丢了一些数据啊。

生产环境也遇到过，我们也是，之前kafka的leader机器宕机了，将follower切换为leader之后，就会发现说这个数据就丢了

所以此时一般是要求起码设置如下4个参数：

1. 给这个topic设置replication.factor参数：这个值必须大于1，要求每个partition必须有至少2个副本
2. 在kafka服务端设置min.insync.replicas参数：这个值必须大于1，这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系，没掉队，这样才能确保leader挂了还有一个follower吧
3. 在producer端设置acks=all：这个是要求每条数据，必须是写入所有replica之后，才能认为是写成功了
4. 在producer端设置retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了

我们生产环境就是按照上述要求配置的，这样配置之后，至少在kafka broker端就可以保证在leader所在broker发生故障，进行leader切换时，数据不会丢失

**生产者会不会弄丢数据**

如果按照上述的思路设置了ack=all，一定不会丢，要求是，你的leader接收到消息，所有的follower都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。

## <a name="7">7.如何保证消息的顺序性？</a>

我举个例子，我们以前做过一个mysql binlog同步的系统，压力还是非常大的，日同步数据要达到上亿。mysql -> mysql，常见的一点在于说大数据team，就需要同步一个mysql库过来，对公司的业务系统的数据做各种复杂的操作。

你在mysql里增删改一条数据，对应出来了增删改3条binlog，接着这三条binlog发送到MQ里面，到消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你楞是换了顺序给执行成删除、修改、增加，不全错了么。

本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。

### RabbitMQ顺序消费

不按顺序消费的情况：

![](https://github.com/lvCmx/study/blob/master/note/面试题/resource/mq不按顺序消费.png)

拆分多个queue，每个queue一个consumer，就是多一些queue而已，确实是麻烦点；或者就一个queue但是对应一个consumer，然后这个consumer内部用内存队列做排队，然后分发给底层不同的worker来处理

### Kafka顺序消费

![](https://github.com/lvCmx/study/blob/master/note/面试题/resource/kafka消息顺序消费.png)

一个topic，一个partition，一个consumer，内部单线程消费，写N个内存queue，然后N个线程分别消费一个内存queue即可

## <a name="8">8.如何解决消息队列的延时以及过期失效问题？</a>

假设你用的是rabbitmq，rabbitmq是可以设置过期时间的，就是TTL，如果消息在queue中积压超过一定的时间就会被rabbitmq给清理掉，这个数据就没了。那这就是第二个坑了。这就不是说数据会大量积压在mq里，而是大量的数据会直接搞丢。

这个情况下，就不是说要增加consumer消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是批量重导，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。

这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入mq里面去，把白天丢的数据给他补回来。也只能是这样了。

假设1万个订单积压在mq里面，没有处理，其中1000个订单都丢了，你只能手动写程序把那1000个订单给查出来，手动发到mq里去再补一次

## <a name="9">9.消息队列满了以后该怎么处理？</a>

如果走的方式是消息积压在mq里，那么如果你很长时间都没处理掉，此时导致mq都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧。

## <a name="10">10.有几百万消息持续积压几小时，说说怎么解决？</a>

几千万条数据在MQ里积压了七八个小时，这个是我们真实遇到过的一个场景，确实是线上故障了，这个时候要不然就是修复consumer的问题，让他恢复消费速度，然后傻傻的等待几个小时消费完毕。这个肯定不能在面试的时候说吧。

一个消费者一秒是1000条，一秒3个消费者是3000条，一分钟是18万条，1000多万条

所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概1小时的时间才能恢复过来

一般这个时候，只能操作临时紧急扩容了，具体操作步骤和思路如下：

 ![](https://github.com/lvCmx/study/blob/master/note/面试题/resource/积压mq消息如何处理.png)

1）先修复consumer的问题，确保其恢复消费速度，然后将现有cnosumer都停掉

2）新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量

3）然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue

4）接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据

5）这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据

6）等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息

